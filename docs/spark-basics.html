<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Spark Basics · GeoTrellis</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Apache Spark is a Scala library for writing distributed processing applications on the JVM."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Spark Basics · GeoTrellis"/><meta property="og:type" content="website"/><meta property="og:url" content="https://geotrellis.io/"/><meta property="og:description" content="Apache Spark is a Scala library for writing distributed processing applications on the JVM."/><meta property="og:image" content="https://geotrellis.io/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://geotrellis.io/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/favicon.png" alt="GeoTrellis"/><h2 class="headerTitleWithLogo">GeoTrellis</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"></ul></nav></div></header></div></div><div class="navPusher singleRowMobileNav"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Spark</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Getting Started</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/getting-started">Getting Started</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Vector</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/vectors">Vectors</a></li><li class="navListItem"><a class="navItem" href="/docs/projection">Projection</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Raster</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/rasters">Rasters</a></li><li class="navListItem"><a class="navItem" href="/docs/tiles">Tiles</a></li><li class="navListItem"><a class="navItem" href="/docs/rastersource">RasterSource</a></li><li class="navListItem"><a class="navItem" href="/docs/rendering-images">Rendering Images</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Layer</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/layer-model">Layer Model</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Spark</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/docs/spark-basics">Basics</a></li><li class="navListItem"><a class="navItem" href="/docs/spark-layer-rdd">Tile Layer RDD</a></li><li class="navListItem"><a class="navItem" href="/docs/spark-indexed-layers">Indexed Layers</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">Spark Basics</h1></header><article><div><span><p>Apache Spark is a Scala library for writing distributed processing applications on the JVM.
Spark is fundamentally built on map-reduce model of computation with added optional persistance intermediate stages.
Spark provides a nice functional-looking API that allows us to treat distributed collections much like we would treat normal collections. However, it is important to develop a mental model of how spark operates in order to effectively write and debug spark applications.</p>
<p><img src="/docs/assets/spark-driver.png" alt="Spark Driver"></p>
<p>When you write and deploy a Spark program it runs on a single JVM termed the &quot;Driver&quot;.
One of the first things a Spark program will do is create a <code>SparkContext</code> which is both your interface to the cluster and job scheduler for task execution.</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">import</span> org.apache.spark.{<span class="hljs-type">SparkConf</span>, <span class="hljs-type">SparkContext</span>}

<span class="hljs-keyword">implicit</span> <span class="hljs-keyword">val</span> sparkContext =
  <span class="hljs-type">SparkContext</span>.getOrCreate(
    (<span class="hljs-keyword">new</span> <span class="hljs-type">SparkConf</span>(loadDefaults =  <span class="hljs-literal">true</span>))
      .setMaster(<span class="hljs-string">"local[*]"</span>)
      .setAppName(<span class="hljs-string">"Demo"</span>)
      .set(<span class="hljs-string">"spark.serializer"</span>, <span class="hljs-string">"org.apache.spark.serializer.KryoSerializer"</span>)
      .set(<span class="hljs-string">"spark.kryo.registrator"</span>, <span class="hljs-string">"geotrellis.spark.store.kryo.KryoRegistrator"</span>)
 )
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="rdd-resilient-distributed-dataset"></a><a href="#rdd-resilient-distributed-dataset" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RDD: Resilient Distributed Dataset</h2>
<p>Spark application represents computation as operations on RDD objects which are owned by the SparkContext.
RDD is a lazy, partitioned, distributed collection. An RDD &quot;contains&quot; partitions of its data.</p>
<p><img src="/docs/assets/spark-rdd.png" alt="Spark RDD"></p>
<h3><a class="anchor" aria-hidden="true" id="actions-and-transformations"></a><a href="#actions-and-transformations" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Actions and Transformations</h3>
<p>The instantiation of the RDD partitions and their data happens on remote executors during task execution.
RDD class instance on the driver captures only the data dependency and reference to work to be performed on that data.</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">import</span> scala.math.random
<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span>

<span class="hljs-keyword">val</span> slices = <span class="hljs-number">32</span>
<span class="hljs-keyword">val</span> n = math.min(<span class="hljs-number">100000</span>L * slices, <span class="hljs-type">Int</span>.<span class="hljs-type">MaxValue</span>).toInt <span class="hljs-comment">// avoid overflow</span>

<span class="hljs-keyword">val</span> countRdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">Int</span>] = sparkContext.parallelize(<span class="hljs-number">1</span> until n, slices)

<span class="hljs-keyword">val</span> mapRdd: <span class="hljs-type">RDD</span>[<span class="hljs-type">Int</span>] = countRdd.map { i =&gt;
  <span class="hljs-keyword">val</span> x = random * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>
  <span class="hljs-keyword">val</span> y = random * <span class="hljs-number">2</span> - <span class="hljs-number">1</span>
  <span class="hljs-keyword">if</span> (x*x + y*y &lt;= <span class="hljs-number">1</span>) <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>
}

<span class="hljs-keyword">val</span> count: <span class="hljs-type">Int</span> = mapRdd.reduce { (a, b) =&gt; a + b}

println(<span class="hljs-string">s"Pi is roughly <span class="hljs-subst">${4.0 * count / (slices - 1)}</span>"</span>)
</code></pre>
<p>Each RDD has a data dependency on its parents each RDD is represented as a Direct Acyclic Graph (DAG).
There are two things you can do with such a DAG: continue building it or evaluate it.</p>
<p><img src="/docs/assets/spark-pi-dag.png" alt="SparkPi DAG"></p>
<ul>
<li><em>Transformation</em> adds a level to the DAG, producing another <code>RDD</code></li>
<li><em>Action</em> Evaluates the DAG and schedules it as Tasks on the cluster</li>
</ul>
<p>In this example call to <code>countRdd.map</code> is a transformation and call to <code>mapRdd.reduce</code> is an action.</p>
<p>One tempting action is <code>.collect</code>. Collecting causes all of the executors to send their records to the driver where it can be &quot;collected&quot; into a single <code>Array</code>. Since the point of spark is to work with datasets larger than memory this action often results in Out of Memory Error on the driver. Caution should be taken when using it in production code.</p>
<h3><a class="anchor" aria-hidden="true" id="forking-dags"></a><a href="#forking-dags" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Forking DAGs</h3>
<p>It is tempting to think that each RDD object represents a specific state of the data as it traverses the DAG, thus they can be used like variables to to build on the computation. For instance if we wanted to validate the count of <code>mapRdd</code> in the next line like so:</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">val</span> samples = mapRdd.count()
</code></pre>
<p>The DAG would look like:</p>
<p><img src="/docs/assets/spark-pi-forked-dag.png" alt="SparkPi Forked Dag"></p>
<p>One imagines that results of <code>mapRdd</code> will be reused for both actions, <strong>this is not true</strong>.
The execution of the program results in two actions: reduce followed by count. Each one will result in DAG evaluation which will resolve all the dependencies and work in <code>mapRdd</code> will be done twice.</p>
<h2><a class="anchor" aria-hidden="true" id="stages"></a><a href="#stages" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Stages</h2>
<p>Spark executes the DAG in stages, of which there are two types: Map and Shuffle.</p>
<h3><a class="anchor" aria-hidden="true" id="map-stage"></a><a href="#map-stage" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Map Stage</h3>
<p><img src="/docs/assets/spark-pipeline-map-stage.png" alt="Spark Map Stage"></p>
<p>At the very lowest level each partition of RDD is evaluated as applying a function to an iterator of records: <a href="https://github.com/apache/spark/blob/0d997e5156a751c99cd6f8be1528ed088a585d1f/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala#L51-L52">MapPartitionsRDD.compute</a></p>
<p>When multiple map transformations follow each-other and all the information required to produce output partition is present in parent partition these transformations can be pipelined, applying the full function stack for each record.</p>
<h3><a class="anchor" aria-hidden="true" id="shuffle-stage"></a><a href="#shuffle-stage" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Shuffle Stage</h3>
<p><img src="/docs/assets/spark-pipeline-shuffle-stage.png" alt="Spark Shuffle Stage"></p>
<p>When the data required for each output record may be spread across several partitions of the parent RDD a shuffle stage is required. This is the case with <code>.groupByKey</code> and <code>.reduceByKey</code> operations.</p>
<p>During the shuffle stage each parent partition is grouped by target partition number, those chunks are written to disk and sent over the network to executors responsible for evaluation of corresponding partition in the next stage.</p>
<p>Often during shuffle every record tries to go where its not. This turns the network into a bottleneck for the spark application. Additionally since the full dataset is always written to disk as part of a shuffle it will place additional strain on the storage.</p>
<p>In reality shuffles are required to do useful work but their number should be minimized to benefit performance. Therefore it is helpful to be aware which types of operations will require a shuffle in your application.</p>
<h2><a class="anchor" aria-hidden="true" id="caching"></a><a href="#caching" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Caching</h2>
<p>This can be avoided by calling either <code>mapRdd.cache</code> or <code>mapRdd.persist(StorageLevel.MEMORY_ONLY)</code>.
The <code>.persist</code> method marks the RDD with a flag that will cause its partitions to be cached on the executors. Persisting an RDD will short-circuit its evaluation and re-use previous results, if available.</p>
<p>Caching should not be used blindly. In many situation its faster to read data from external source and duplicate transformations. After all caching has its own overhead. Memory-only RDD persistance competes with memory required to perform the transformation and on-disk persistance involves a whole new storage subsystem in your job.</p>
<p>Typically only stages used multiple times that have been results of several shuffles should be cached.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/layer-model"><span class="arrow-prev">← </span><span>Layer Model</span></a><a class="docs-next button" href="/docs/spark-layer-rdd"><span>Tile Layer RDD</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#rdd-resilient-distributed-dataset">RDD: Resilient Distributed Dataset</a><ul class="toc-headings"><li><a href="#actions-and-transformations">Actions and Transformations</a></li><li><a href="#forking-dags">Forking DAGs</a></li></ul></li><li><a href="#stages">Stages</a><ul class="toc-headings"><li><a href="#map-stage">Map Stage</a></li><li><a href="#shuffle-stage">Shuffle Stage</a></li></ul></li><li><a href="#caching">Caching</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/favicon.png" alt="GeoTrellis" width="66" height="58"/></a><div><h5>Community</h5><a href="/en/users.html">User Showcase</a><a target="_blank" rel="noreferrer noopener">Gitter.im</a></div><div><h5>More</h5><a href="undefined">Blog</a><a class="github-button" href="" data-icon="octicon-star" data-count-href="/locationtech/geotrellis/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><section class="copyright">Copyright © 2020 Azavea</section></footer></div></body></html>